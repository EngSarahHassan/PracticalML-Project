---
title: "Analysis of Accelerometers Data"
author: "Sarah"
date: "9/17/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction   

This project uses data recorded from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. These participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  

The goal is to predict the manner in which the participants did the exercise >> classe variable 

## Getting Data  

The training and testing sets are downloaded from the online source as below:  

```{r}
set.seed(111)
```

```{r cache=TRUE}
if(!file.exists("data")){
        dir.create("`data")
        download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile = "data/training.csv")
        download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile = "data/testing.csv")        
}
training<-read.csv("data/training.csv")
testing<-read.csv("data/testing.csv")

```


```{r echo=FALSE}
nCols<-dim(training)[2]
nRows<-dim(training)[1]
```

# Exploring the training set
In order to build the model we should first explore the training set. The training set consists of `r nCols` columns and `r nRows` rows.  

```{r}
checkNAs<-function(v,len){
        s1<-sum(is.na(v)*1)/len
        s2<-0
        if(class(v)=="factor"){
          s2<-sum((v=="")*1)/len
        }
        max(s1,s2)
}
amountNAs<-sapply(training,FUN = checkNAs,nRows)
colsNAs<-amountNAs[which(amountNAs>0)]
```

1- By looking at the data, we first noticed `r length(colsNAs)` columns with high percentage of missing values so we decided to track them and exclude them from the data set:  

```{r}
colsNAs
```

```{r}

trainingNew<-training[,!(names(training) %in% names(colsNAs))]
testingNew<-testing[,!(names(testing) %in% names(colsNAs))]
```

Now we are left with 60 columns  
2- we will exclude the first 7 columns as they are related to participants information and other information not usefull to be used as predictors.  
 
```{r}
excludedCols<-names(trainingNew)[1:7]
excludedCols
```


```{r}
trainingNew<-trainingNew[,!(names(trainingNew) %in% excludedCols)]
testingNew<-testingNew[,!(names(testingNew) %in% excludedCols)]

```

Now we have `r dim(trainingNew)[2]` columns that will try to fit model with  

# Building the models  

## Extracting validation set   

We split the training set to training and validation. The testing set is left for final testing on unseen data. The validation set is used to evaluate accuracy. 

```{r cache=TRUE}
library(caret)
inTrain <- createDataPartition(y = trainingNew$classe, p = 0.8, list = FALSE)
trainingNew <- trainingNew[inTrain, ]
validation <- trainingNew[-inTrain, ]

dim(trainingNew);dim(validation)
```

## Define CV function  

This cross validation function will be used in the train function for all models 

```{r cache=T}
fitControl <- trainControl(method = "cv", number = 5, returnResamp = "all")
```

## Train model :logistic regression with boosting 

```{r cache=TRUE}
mod.LogitBoost<- train(classe ~ ., data = trainingNew, method = "LogitBoost"
              , trControl = fitControl)
```

```{r}
plot(mod.LogitBoost)
```
The LogitBoost shows relatively good accuracy over all cross validation sets  

## Train model :classification tree  

```{r cache=TRUE}
mod.rpart<- train(classe ~ ., data = trainingNew, method = "rpart",
                 trControl = fitControl)
```

```{r}
plot(mod.rpart)
```

The rpart model shows worse results than LogitBoost over all the cross validation sets.  

## Evaluate on validation set  
### LogitBoost  

```{r cache=TRUE}
pred.valid.LogitBoost<-predict(mod.LogitBoost,validation)
confusionMatrix(pred.valid.LogitBoost,validation$classe)
```
### classification tree  

```{r cache=TRUE}
pred.valid.rpart<-predict(mod.rpart,validation)
confusionMatrix(pred.valid.rpart,validation$classe)
```

# Prediction on test set  

## LogitBoost  

```{r}
pred.test.LogitBoost<-predict(mod.LogitBoost,testingNew)

```

The â€œlogistic regression with boosting" model give a very high accuracy (>90%) on validation and testing set with only few classes predicted wrong.

## In sample vs. out of sample error of LogitBoost (best accuracy model)

```{r cache=TRUE}
#In Sample
pred.train.LogitBoost<-predict(mod.LogitBoost,trainingNew)
NAS<- which(is.na(pred.train.LogitBoost))
1-sum((pred.train.LogitBoost[-NAS]==trainingNew[-NAS,'classe'])*1)/length(pred.train.LogitBoost)

#Out Sample
NAS<- which(is.na(pred.valid.LogitBoost))
1-sum((pred.valid.LogitBoost[-NAS]==validation[-NAS,'classe'])*1)/length(pred.valid.LogitBoost)
```
The out of sample error is higher than the in sample error because it was tested on new unseen data but generaly it is very low 
